{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook is developed for PICES 2022\n",
    "\n",
    "### CODAR\n",
    "\n",
    "CODAR is a technology that allows the measurement of surface ocean current velocities at a distance using the Doppler shift of reflected radio waves. The principle of Bragg scattering dictates that virtually all of the radio signal received by a CODAR antenna is being reflected back from ocean waves of a particular wavelength. The reflected signal is Doppler-shifted to higher or lower frequencies, depending on whether the ocean wave is moving, respectively, toward or away from the antenna. In the absence of ocean currents, this Doppler shift gives a measurement of the wave spee. If ocean currents are present, however, the Doppler shift gives a measurement of the wave speed PLUS the speed of the ocean current towards or away from the antenna. Because the speed of deep-water waves is known for a given wavelength, the wave speed can be subtracted from the total measured speed, resulting in a value for the speed of the ocean current alone.\n",
    "\n",
    "The strait of Georgia CODAR system used by ONC is a 25 MHz model. A radio frequency of 25 MHz corresponds to a radio wavelength of 12 meters, so the CODAR system is sensitive to ocean wavelengths of 6 meter. The equation used in calculating ocean current velocities from the Bragg-scattering signal assume that the ocean waves being measured are \"deep-water\" waves. In other words, their wavelengths are less than twice the water depth. As depth decreases beyong this point, the waves will increasingly take on the character of \"shallow-water\" waves. Thus that for a 25 MHz system, ocean currents measured in water depths of less than 3 meters should be viewd with suspiction.  \n",
    "\n",
    "\n",
    "### CODAR stations and combiners\n",
    "\n",
    "Each CODAR systems measures the radial velocities of ocean surface currents, toward and away from the station's attenna. Four stations (VCOL, VION, VGPT, VATK) in the Strait of Georgia  have been placed in locations such that their areas of coverage overlap considerably. In the overlapping regions, it is possible to combine the radial data from at least two stations into total ocean currents. \n",
    "\n",
    "In the strait of Georgia, four antennas have been gradually installed since 2011. \n",
    "\n",
    "\n",
    "Note that the range of high-frequency radio waves is greatly reduced over fresh water compared to the range over salty, highly-conductive water. For this reason, during the spring freshet (June 2018), when the surface water in the strait of Gerogia is much less sality, less coverage of total ocean current would be expected.\n",
    "\n",
    "\n",
    "### Grid\n",
    "\n",
    "The total files organize the data into grids with y,x and time dimensions, while the radials files use arrays for the current data, with dimensions of time and index, where index links the currents to their lat/lon sampling locations. The x,y dimensions are the grid centers relative to the grid origin.\n",
    "\n",
    "The radial files from combiner stations will contain data from multiple radials stations; the platform_name variable will supply the station name / call sign (i.e. VION) for each time step. \n",
    "\n",
    "\n",
    "### Data description\n",
    "\n",
    "MAT file data structures and definitions\n",
    "\n",
    "This section describes the MAT file structure in detail. HFRProgs's README and code also documents these structures, so only relevant ones will be listed in detail.\n",
    "\n",
    "**Totals**\n",
    "\n",
    "1. U/V: surface current total velocities calculated from the Radials structure\n",
    "2. LonLat,2 vector containing longitude and latitude pairs for each U/V value\n",
    "3. Timestamp: date in number format.\n",
    "\n",
    "### Data included: June and Nov. 2018 of total vectors in formats of *.mat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdf5storage import savemat\n",
    "from hdf5storage import loadmat\n",
    "import os\n",
    "from datetime import timedelta  \n",
    "import scipy.stats as stats\n",
    "import matplotlib.mlab as mlab\n",
    "from matplotlib.pyplot import cm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.matlib\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import glob\n",
    "from datetime import datetime\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Ellipse\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "%matplotlib nbagg\n",
    "cmap = matplotlib.cm.jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matlab2datetime(matlab_datenum):\n",
    "    day = datetime.fromordinal(int(matlab_datenum))\n",
    "    dayfrac = timedelta(days=matlab_datenum%1) - timedelta(days = 366)\n",
    "    return day + dayfrac\n",
    "\n",
    "\n",
    "def vector_log(U_raw,V_raw):\n",
    "    SpeedL=np.log10(np.sqrt(U_raw**2+V_raw**2))\n",
    "    Theta=np.arctan(V_raw/U_raw)\n",
    "    UL=np.sign(U_raw)*SpeedL*np.abs(np.cos(Theta))\n",
    "    VL=np.sign(V_raw)*SpeedL*np.abs(np.sin(Theta))\n",
    "    return UL, VL    \n",
    "\n",
    "def plotBasemap(Latmin,Latmax,Lonmin,Lonmax,Title):   \n",
    "    \n",
    "    M=Basemap(projection='merc', llcrnrlat=Latmin,urcrnrlat=Latmax,llcrnrlon=Lonmin,\n",
    "              urcrnrlon=Lonmax,resolution='h',area_thresh = 0.01)    \n",
    "    \n",
    "    M.drawparallels(np.arange(48.85,49.37,0.1) ,labels=[1,0,0,0]);\n",
    "    M.drawmeridians(np.arange(-123.8,-122.95,0.3),labels=[0,0,0,1]);\n",
    "    M.drawmapboundary()\n",
    "    M.fillcontinents(color='grey')\n",
    "    \n",
    "    \n",
    "    plt.title(Title)\n",
    "    Stationnames=['VION','VGPT','VATK','VCOL'] \n",
    "    for i,Istation in enumerate((Stationnames)):\n",
    "        XO,YO=M(orig_lon[Istation],orig_lat[Istation])\n",
    "        M.plot(XO,YO,'r^',markersize=10)\n",
    "        plt.text(XO,YO,Istation,fontsize=14)\n",
    "        plt.title(Title,fontsize=14)\n",
    "    return M    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data files for the specific month and set up your own basedir \n",
    "month='Nov'\n",
    "basedir='/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/'\n",
    "datafolder=basedir+'Data_PICES2022/'+month+'2018_totals_mat'\n",
    "datafiles=sorted(glob.glob(datafolder+'/*.mat'))\n",
    "savefigurefolder=basedir+'savefigures/'+month+'/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/Data_PICES2022/Nov2018_totals_mat/StraitofGeorgia_StraitofGeorgiaCODARSystem_OceanographicRadarSystem_20181101T000000.000Z_20181115T210000.000Z-Totals_Radials_Clean.mat',\n",
       " '/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/Data_PICES2022/Nov2018_totals_mat/StraitofGeorgia_StraitofGeorgiaCODARSystem_OceanographicRadarSystem_20181115T220000.000Z_20181130T050000.000Z-Totals_Radials_Clean.mat',\n",
       " '/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/Data_PICES2022/Nov2018_totals_mat/StraitofGeorgia_StraitofGeorgiaCODARSystem_OceanographicRadarSystem_20181130T060000.000Z_20181130T230000.000Z-Totals_Radials_Clean.mat']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Longitude and latitude of the four radar sites for plotting sites\n",
    "orig_lon={}\n",
    "orig_lat={}\n",
    "\n",
    "orig_lon['VION']=-123.2053833\n",
    "orig_lon['VCOL']=-123.1718833\n",
    "orig_lon['VGPT']=-123.291\n",
    "orig_lon['VATK']=-123.2644333\n",
    "orig_lon['VDIG']=-130.4254500\n",
    "orig_lon['VRID']=-130.3346833\n",
    "\n",
    "orig_lat['VATK']=49.3300667\n",
    "orig_lat['VCOL']=49.01805\n",
    "orig_lat['VGPT']=48.87365\n",
    "orig_lat['VION']=49.2158667\n",
    "orig_lat['VDIG']=54.2625333\n",
    "orig_lat['VRID']=54.2346333\n",
    "\n",
    "#basemap boundary\n",
    "stationnames=['VION','VGPT','VATK','VCOL'] \n",
    "latmin = 48.85\n",
    "latmax = 49.39\n",
    "lonmin =-123.8\n",
    "lonmax =-122.9\n",
    "\n",
    "latpar_min=round(latmin*10)//10\n",
    "latpar_max=round(latmax*10)//10\n",
    "lonpar_min=round(lonmin*10)//10\n",
    "lonpar_max=round(lonmax*10)//10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/Data_PICES2022/Nov2018_totals_mat/StraitofGeorgia_StraitofGeorgiaCODARSystem_OceanographicRadarSystem_20181101T000000.000Z_20181115T210000.000Z-Totals_Radials_Clean.mat\n",
      "1\n",
      "/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/Data_PICES2022/Nov2018_totals_mat/StraitofGeorgia_StraitofGeorgiaCODARSystem_OceanographicRadarSystem_20181115T220000.000Z_20181130T050000.000Z-Totals_Radials_Clean.mat\n",
      "2\n",
      "/Volumes/GoogleDrive/My Drive/ConferencesWorkshops/PICES2022_demo/Data_PICES2022/Nov2018_totals_mat/StraitofGeorgia_StraitofGeorgiaCODARSystem_OceanographicRadarSystem_20181130T060000.000Z_20181130T230000.000Z-Totals_Radials_Clean.mat\n"
     ]
    }
   ],
   "source": [
    "# load three mat files for the specific month and then \n",
    "u=[]\n",
    "v=[]\n",
    "date=[]\n",
    "for i in range(len(datafiles)):\n",
    "    print (i)\n",
    "    print (datafiles[i])\n",
    "    datai=loadmat(datafiles[i])\n",
    "    num_hours=len(datai['Totals'])\n",
    "    for j in range(num_hours):\n",
    "        \n",
    "        #get u and v\n",
    "        dataj=datai['Totals'][j][0]\n",
    "        uj=dataj['U']\n",
    "        vj=dataj['V']\n",
    "        u=np.append(u,uj)\n",
    "        v=np.append(v,vj)\n",
    "        \n",
    "        #get date\n",
    "        timestamp=dataj['TimeStamp'][0][0]+0.0000000001\n",
    "        datej=str(matlab2datetime(np.float(timestamp)))[0:13]\n",
    "        date=np.append(date,datej)\n",
    "    # get lon and lat    \n",
    "    lonlat=dataj['LonLat']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put u and v into Pandas dataframe\n",
    "lon=lonlat[:,0]\n",
    "lat=lonlat[:,1]\n",
    "num_hours=len(date)\n",
    "u_df=pd.DataFrame(u.reshape(num_hours,len(u)//num_hours).T,index=[lon,lat],columns=date)\n",
    "v_df=pd.DataFrame(v.reshape(num_hours,len(u)//num_hours).T,index=[lon,lat],columns=date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>2018-11-01 00</th>\n",
       "      <th>2018-11-01 01</th>\n",
       "      <th>2018-11-01 02</th>\n",
       "      <th>2018-11-01 03</th>\n",
       "      <th>2018-11-01 04</th>\n",
       "      <th>2018-11-01 05</th>\n",
       "      <th>2018-11-01 06</th>\n",
       "      <th>2018-11-01 07</th>\n",
       "      <th>2018-11-01 08</th>\n",
       "      <th>2018-11-01 09</th>\n",
       "      <th>...</th>\n",
       "      <th>2018-11-30 14</th>\n",
       "      <th>2018-11-30 15</th>\n",
       "      <th>2018-11-30 16</th>\n",
       "      <th>2018-11-30 17</th>\n",
       "      <th>2018-11-30 18</th>\n",
       "      <th>2018-11-30 19</th>\n",
       "      <th>2018-11-30 20</th>\n",
       "      <th>2018-11-30 21</th>\n",
       "      <th>2018-11-30 22</th>\n",
       "      <th>2018-11-30 23</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-123.015064</th>\n",
       "      <th>48.788191</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-123.001455</th>\n",
       "      <th>48.788148</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-122.987846</th>\n",
       "      <th>48.788104</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-122.919804</th>\n",
       "      <th>48.787858</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-122.906195</th>\n",
       "      <th>48.787804</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-123.741076</th>\n",
       "      <th>49.426560</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-123.727292</th>\n",
       "      <th>49.426604</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-123.713508</th>\n",
       "      <th>49.426645</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-123.699723</th>\n",
       "      <th>49.426685</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>-123.685939</th>\n",
       "      <th>49.426724</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2532 rows × 720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       2018-11-01 00  2018-11-01 01  2018-11-01 02  \\\n",
       "-123.015064 48.788191            NaN            NaN            NaN   \n",
       "-123.001455 48.788148            NaN            NaN            NaN   \n",
       "-122.987846 48.788104            NaN            NaN            NaN   \n",
       "-122.919804 48.787858            NaN            NaN            NaN   \n",
       "-122.906195 48.787804            NaN            NaN            NaN   \n",
       "...                              ...            ...            ...   \n",
       "-123.741076 49.426560            NaN            NaN            NaN   \n",
       "-123.727292 49.426604            NaN            NaN            NaN   \n",
       "-123.713508 49.426645            NaN            NaN            NaN   \n",
       "-123.699723 49.426685            NaN            NaN            NaN   \n",
       "-123.685939 49.426724            NaN            NaN            NaN   \n",
       "\n",
       "                       2018-11-01 03  2018-11-01 04  2018-11-01 05  \\\n",
       "-123.015064 48.788191            NaN            NaN            NaN   \n",
       "-123.001455 48.788148            NaN            NaN            NaN   \n",
       "-122.987846 48.788104            NaN            NaN            NaN   \n",
       "-122.919804 48.787858            NaN            NaN            NaN   \n",
       "-122.906195 48.787804            NaN            NaN            NaN   \n",
       "...                              ...            ...            ...   \n",
       "-123.741076 49.426560            NaN            NaN            NaN   \n",
       "-123.727292 49.426604            NaN            NaN            NaN   \n",
       "-123.713508 49.426645            NaN            NaN            NaN   \n",
       "-123.699723 49.426685            NaN            NaN            NaN   \n",
       "-123.685939 49.426724            NaN            NaN            NaN   \n",
       "\n",
       "                       2018-11-01 06  2018-11-01 07  2018-11-01 08  \\\n",
       "-123.015064 48.788191            NaN            NaN            NaN   \n",
       "-123.001455 48.788148            NaN            NaN            NaN   \n",
       "-122.987846 48.788104            NaN            NaN            NaN   \n",
       "-122.919804 48.787858            NaN            NaN            NaN   \n",
       "-122.906195 48.787804            NaN            NaN            NaN   \n",
       "...                              ...            ...            ...   \n",
       "-123.741076 49.426560            NaN            NaN            NaN   \n",
       "-123.727292 49.426604            NaN            NaN            NaN   \n",
       "-123.713508 49.426645            NaN            NaN            NaN   \n",
       "-123.699723 49.426685            NaN            NaN            NaN   \n",
       "-123.685939 49.426724            NaN            NaN            NaN   \n",
       "\n",
       "                       2018-11-01 09  ...  2018-11-30 14  2018-11-30 15  \\\n",
       "-123.015064 48.788191            NaN  ...            NaN            NaN   \n",
       "-123.001455 48.788148            NaN  ...            NaN            NaN   \n",
       "-122.987846 48.788104            NaN  ...            NaN            NaN   \n",
       "-122.919804 48.787858            NaN  ...            NaN            NaN   \n",
       "-122.906195 48.787804            NaN  ...            NaN            NaN   \n",
       "...                              ...  ...            ...            ...   \n",
       "-123.741076 49.426560            NaN  ...            NaN            NaN   \n",
       "-123.727292 49.426604            NaN  ...            NaN            NaN   \n",
       "-123.713508 49.426645            NaN  ...            NaN            NaN   \n",
       "-123.699723 49.426685            NaN  ...            NaN            NaN   \n",
       "-123.685939 49.426724            NaN  ...            NaN            NaN   \n",
       "\n",
       "                       2018-11-30 16  2018-11-30 17  2018-11-30 18  \\\n",
       "-123.015064 48.788191            NaN            NaN            NaN   \n",
       "-123.001455 48.788148            NaN            NaN            NaN   \n",
       "-122.987846 48.788104            NaN            NaN            NaN   \n",
       "-122.919804 48.787858            NaN            NaN            NaN   \n",
       "-122.906195 48.787804            NaN            NaN            NaN   \n",
       "...                              ...            ...            ...   \n",
       "-123.741076 49.426560            NaN            NaN            NaN   \n",
       "-123.727292 49.426604            NaN            NaN            NaN   \n",
       "-123.713508 49.426645            NaN            NaN            NaN   \n",
       "-123.699723 49.426685            NaN            NaN            NaN   \n",
       "-123.685939 49.426724            NaN            NaN            NaN   \n",
       "\n",
       "                       2018-11-30 19  2018-11-30 20  2018-11-30 21  \\\n",
       "-123.015064 48.788191            NaN            NaN            NaN   \n",
       "-123.001455 48.788148            NaN            NaN            NaN   \n",
       "-122.987846 48.788104            NaN            NaN            NaN   \n",
       "-122.919804 48.787858            NaN            NaN            NaN   \n",
       "-122.906195 48.787804            NaN            NaN            NaN   \n",
       "...                              ...            ...            ...   \n",
       "-123.741076 49.426560            NaN            NaN            NaN   \n",
       "-123.727292 49.426604            NaN            NaN            NaN   \n",
       "-123.713508 49.426645            NaN            NaN            NaN   \n",
       "-123.699723 49.426685            NaN            NaN            NaN   \n",
       "-123.685939 49.426724            NaN            NaN            NaN   \n",
       "\n",
       "                       2018-11-30 22  2018-11-30 23  \n",
       "-123.015064 48.788191            NaN            NaN  \n",
       "-123.001455 48.788148            NaN            NaN  \n",
       "-122.987846 48.788104            NaN            NaN  \n",
       "-122.919804 48.787858            NaN            NaN  \n",
       "-122.906195 48.787804            NaN            NaN  \n",
       "...                              ...            ...  \n",
       "-123.741076 49.426560            NaN            NaN  \n",
       "-123.727292 49.426604            NaN            NaN  \n",
       "-123.713508 49.426645            NaN            NaN  \n",
       "-123.699723 49.426685            NaN            NaN  \n",
       "-123.685939 49.426724            NaN            NaN  \n",
       "\n",
       "[2532 rows x 720 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Temporal availability\n",
    "\n",
    "Every hour has different number of grid points that has available data. Before any use of these data, it will be helpful to check the availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot available number of data grid points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Spatial Availability \n",
    "\n",
    "Spatial availability=number of available hours/total hours of that month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select one of the grid cell and then plot out the ocean current map\n",
    "# For instance, Plot the total currents that has maximum data coverage during that month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) Plot UV \n",
    "\n",
    "- UV time series\n",
    "- UV map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) spectral analysis\n",
    "\n",
    "1. select one grid cell, which has maximum spatial data avaialability\n",
    "2. rotary spectra analysis for u and v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to adjust nfft based on the length of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot rotary spectra, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5) PCA analysis\n",
    "\n",
    "\n",
    "Sources\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/cross_decomposition/plot_pcr_vs_pls.html#\n",
    "\n",
    "https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\n",
    "\n",
    "Steps of calculating PCA: \n",
    "\n",
    "1. Subtract the mean of each value;\n",
    "2. Calculate the Covariance Matrix: The covariance matrix is a square matrix donoting the covariance of elements with each other. The covariance of an element with itself is nothing but just its variance;\n",
    "3. Compute the eigenvalues and eigenvectors:\n",
    "\n",
    "    3.1 The eigenvectors of the covariance matrix we get are orthononal to each other and each vector represents a principle axis;    \n",
    "    3.2 A higher eigenvalue corresponds to a higher variability. Hence the principal axis with the higher eigenvalue will be an axis captuting higher variability in the data;    \n",
    "    3.3 Orthogonal means the vectors are mutually perpendicular to each other; \n",
    "    \n",
    "4. Sort eigenvalues in descending order:\n",
    "\n",
    "    4.1 Sort the eigenvalues in the descending order along with their corrsponding eigenvectors;    \n",
    "    4.2 Each column in the eigen vector-matrix corresponds to a principal compoment, so arranging them in desciending order of their eigenvalue will automatically arrange the principal component in desciending order of their variability;    \n",
    "    4.3 So the first column in the rearranged eigen vector-matrix will be a principle compionent that captures the highest variability.\n",
    "    \n",
    "5. Select a subset from the rearraged eigenvalue matrix\n",
    "\n",
    "6. Transform the data. Transform the data by having a dot product between the transpose of the eigenvector subset and the transpose of the mean-centered data. \n",
    "\n",
    "\n",
    "PC actually is the direction with variance scaled. Direction can be decided by the eigenvectors of covariance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input UU and VV data time series at a certain grid cell in format of dataframe\n",
    "def pca_data(UU,VV,N_std):\n",
    "    XX=pd.DataFrame({'u':UU,'v':VV}).values\n",
    "    XX_meaned=XX-np.mean(XX,axis=0)\n",
    "    Ppca=PCA(n_components=2).fit(XX_meaned)\n",
    "    \n",
    "    Angle=np.arctan(Ppca.components_[0,1]/Ppca.components_[0,0])*180/np.pi+180\n",
    "    Ellipse_norm=N_std*np.sqrt(Ppca.explained_variance_)* Ppca.components_.T #normalized by number of N_std\n",
    "    Ell_radius_x=np.sqrt(Ellipse_norm[0,0]**2+Ellipse_norm[1,0]**2)\n",
    "    Ell_radius_y=np.sqrt(Ellipse_norm[0,1]**2+Ellipse_norm[1,1]**2)\n",
    "       \n",
    "    return XX_meaned,Angle,Ell_radius_x,Ell_radius_y,Ppca\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1,v0, arrowprops=arrowprops)\n",
    "    \n",
    "def plot_ellipse_grid(PCA,N_std,X_demeaned,Ellipse_x,Ellipse_y,Angle,Tit,Col):\n",
    "\n",
    "    plt.scatter(X_demeaned[:, 0], X_demeaned[:, 1], color=Col,alpha=0.2,label='De_meaned')    \n",
    "    e = Ellipse(xy = (0,0), width = Ellipse_x * 2, height = Ellipse_y * 2, angle=Angle,\n",
    "           edgecolor='blue',facecolor='none',joinstyle='bevel',\n",
    "                     linewidth=1)\n",
    "\n",
    "    for Length, Vector in zip(PCA.explained_variance_, PCA.components_):\n",
    "        VV = Vector  * np.sqrt(Length)*N_std\n",
    "        draw_vector(PCA.mean_, PCA.mean_ +VV)\n",
    "    \n",
    "    \n",
    "    Label_x=-100+13;Label_y=100-13;Scale=5\n",
    "    e2 = Ellipse(xy = (Label_x,Label_y), width =Scale*2, height =Scale*2, angle=90,\n",
    "               edgecolor=Col,facecolor='none',label=r'$2\\sigma$',joinstyle='bevel',\n",
    "                         linewidth=1)\n",
    "    plt.plot([Label_x,Label_x+Scale],[Label_y,Label_y],color='black')\n",
    "    ax.add_artist(e2);plt.text(Label_x-Scale-Scale/2,Label_y+Scale+Scale/2,'5cm/s') \n",
    "    \n",
    "    ax.add_artist(e);plt.xlim([-100,100]);plt.ylim([-100,100])\n",
    "    ax.legend(loc=3,fontsize=8)\n",
    "    ax.set_xlabel('U [cm/s]',fontsize=11)\n",
    "    ax.set_ylabel('V [cm/s]',fontsize=11)\n",
    "    plt.axvline(x = 0,color='black',alpha=0.3)\n",
    "    plt.axhline(y = 0,color='black',alpha=0.3)\n",
    "    plt.text(75,-50,'$2\\sigma$',fontsize=15);plt.title(Tit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# select u and v from one of the grid cells then plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Save one grid cell data and then load into matlab to run t-tide "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7) Calculate and plot the tidal analysis for one grid/site [u,v]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tidal analysis results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######\n",
    "# load major, minor and angle of ellipse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot u and v tide\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ellipse\n",
    "\n",
    "# Plot preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
